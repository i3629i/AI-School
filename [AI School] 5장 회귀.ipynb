{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회귀(Refression)\n",
    "   \n",
    "1. 지도 학습을 이해하기\n",
    "    1. Supervised 와 Unsupervised를 이해하기\n",
    "    \n",
    "1. 회귀의 다양한 방식 알아보기\n",
    "    1. 변수의 개수\n",
    "    1. Cost 함수\n",
    "    1. 경사하강법\n",
    "1. Regression 의 응용\n",
    "    1. Lasso Regression\n",
    "    1. Ridge Regression\n",
    "    1. Elastic Net Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 지도학습 이해하기\n",
    "\n",
    "#### 지도학습\n",
    "특정 input 값에 정답(label)이 있는 데이터가 주어진다.\n",
    "\n",
    "Regression : 어떤 input 값을 특정 output값에 대응시켜주는 과정(\n",
    "* ex) 면적의 값을 input이라고 하고  땅 가격을 output일때 어느 면적을 입력하면 기존의 데이터를 통한 학습을 통해 방정식을 만들어 땅 가격을 예측 할 수 있음.\n",
    "\n",
    "Classification : 주어진 input이 어느 카테고리에 있는지 판별\n",
    "\n",
    "![Regression](https://user-images.githubusercontent.com/50629716/61850788-189f7c00-aef0-11e9-9cdf-cabfa5cb8cf0.JPG)\n",
    "\n",
    "\n",
    "#### Cost Function\n",
    "\n",
    "모델식에 가장맞는 가중치를 찾기위해서\n",
    "\n",
    "h(x) = Θ(Theta) + Θ(Theta)x   /// ax+b 선형방정식\n",
    "\n",
    "이때 Cost Function을 찾는 방법이 최소제곱법\n",
    "* J(Θ0,Θ1) = 1/2m ∑(h(x) - y)^2 \n",
    "* Θ는 계수(가중치) x는 설명변수로 \n",
    "\n",
    "\n",
    "ex)\n",
    "* x =  [0.5, 0.8, 1.1, 1.5] // 역까지의 거리,   \n",
    "* y =  [8.7, 7.5, 7.1, 6.8] // 가격 (단위 : 십만원)\n",
    "* 가중치 Θ0 Θ1 을 구하여라 근사 값을 구해줘야함\n",
    "\n",
    "1/2*4{ (Θ0+0.5Θ1 - 8.7)^2 + (Θ0+0.8Θ1 - 7.5)^2 + (Θ0+1.1Θ1 - 7.1)^2 + (Θ0+1.5Θ1 - 6.8)^2 = D\n",
    "\n",
    "    ∂D / ∂Θ0  = 0 //  Θ0에 대해서 편미분 하면 0이나와야함\n",
    "    ∂D / ∂Θ1  = 0 //  Θ1에 대해서 편미분 하면 0이나와야함\n",
    "\n",
    "편미분한 식으로 연립방정식을 해주면 h(x) = Θ0 + Θ1X 에서 Θ0,Θ1을 구할수 있음\n",
    "Θ0 + Θ1X 이 방정식이 선형회귀\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "경사하강법\n",
    "\n",
    "Cost함수 그래프를 그리면 2차원 함수 포물선이 그려진다. \n",
    "\n",
    "D = 2차식 ( Θ0 , Θ1) 에의한 2차식 포물선이 나옴 // J(Θ0,Θ1) = 1/2m ∑(h(x) - y)^2 \n",
    "\n",
    "\n",
    "    *Θ0 ≒ Θ0 - α(Learning rate)* ∂J(Θ0,Θ1) / ∂Θ0\n",
    "    *Θ1 ≒ Θ1 - α(Learning rate)* ∂J(Θ0,Θ1) / ∂Θ1 \n",
    "    \n",
    "https://medium.com/@peteryun/ml-%EB%AA%A8%EB%91%90%EB%A5%BC-%EC%9C%84%ED%95%9C-tensorflow-3-gradient-descent-algorithm-%EA%B8%B0%EB%B3%B8-c0688208fc59\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "\n",
    "#### Lasso Regression\n",
    "\n",
    "Cost Function에 L1 정규화한 항을 추가\n",
    "\n",
    "    *정규화한항 : Overfitting 을 막기 위해서 사용하는 기법, Θ를 0 에 가깝게 만들어 모델의 복잡성을 줄임\n",
    "    \n",
    "1/2m ∑(h(x) - y)^2 ＋λ/2 ∑|Θj| (1부터 n까지의 합) \n",
    "\n",
    "#### Ridge Regression\n",
    "\n",
    "L2 정규화 항을 추가\n",
    "\n",
    "1/2m ∑(h(x) - y)^2 ＋λ/2 ∑Θj^2 (1부터 n까지의 합) \n",
    "\n",
    "    \n",
    "    * Lasso 는 중요한 몇 개변수 빼고 다른 계수를 다 0으로 만들어 버린다\n",
    "    * Ridge 는 계수를 0에 가깝게 만들지만 0은 아니라 복잡성이 있을 수 있음.\n",
    "    \n",
    "#### Elastic Net Regression\n",
    "    \n",
    "두 개를 모두 다써서 만듬 좀더 안정적"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

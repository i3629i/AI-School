{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 심층 신경망 과 최적화\n",
    "1. 딥러닝 학습의 문제점화 해결방법\n",
    "    1. 기울기 손실(Gradient Vanishing)\n",
    "    1. 가중치 초기화\n",
    "1. 최적화 알고\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기울기 손실(Gradient Vanishing)\n",
    "더 깊은 Layer 에서는 더 학습이 잘되는거 아닌가? 하지만 기울기 손실이 발생한다.\n",
    "\n",
    "기울기 손실이란? : Out에 나오는 값과 멀이질 수록 학습이 모호하게 진행됨 Sigmoid함수 에서의 1보다 작으면 0에 가까워진다는 이유로 계속 0에 가까워져 기울기가 모호해짐(역전파시 0에 수렴하는게 많음)\n",
    "![gradient](https://user-images.githubusercontent.com/50629716/62521649-0536ce00-b86b-11e9-8737-0a84b67d27db.PNG)\n",
    "* Sigmoid\n",
    "도함수의 최대값은 0.25\n",
    "* ReLU\n",
    "도함수 값이 0이나 1이기 때문에 컴퓨터 측면에서 경제적 학습도 빠름\n",
    "* Leaky ReLU\n",
    "0보다 작은 경우 ReLU에서 신경이 죽어버리는 현상 극복\n",
    "* Tanh\n",
    "함수값의 범위가(-1,1), 도함수의 최댓값 1\n",
    "\n",
    "#### 가중치 초기화\n",
    "![sigmoid](https://user-images.githubusercontent.com/50629716/62523905-b2abe080-b86f-11e9-8c7c-a4cc098a9bf4.PNG)\n",
    "* t = wx + b 에서 시그모이드 함수 미분시 t가 5만 넘어도 0에 수렴 -> 기울기 소실 문제 발생\n",
    "* 가중치w를 0으로 초기화하면 두번째 층 레이어 의 뉴런이 모두 같은 값이 전달됨\n",
    "![weight initalization](https://user-images.githubusercontent.com/50629716/62524407-86449400-b870-11e9-9296-59c74e95ab74.PNG)\n",
    "\n",
    "**표준편차를 0.01로 하는 정규분포 형태로 초기화 했을 때 0.5 중심으로 모여있음.**\n",
    "\n",
    "1. Xavier initalization (RBM보다 좋은 방법이 생김)\n",
    "하나의 노드에 몇개의 입력이고 몇개의 출력인가에 맞게 비례해 초기값을 주는 방법.\n",
    "\n",
    "    * 표준 정규 분포를 입력 개수의 제곤근으로 나누어줌.\n",
    "    * W = np.random.randn(n_input,n_output)/sqrt(n_input)\n",
    "![weight1](https://user-images.githubusercontent.com/50629716/62525026-a0cb3d00-b871-11e9-8ee7-3dcbff1350df.PNG)\n",
    "![w2](https://user-images.githubusercontent.com/50629716/62525027-a0cb3d00-b871-11e9-90be-617c885ab639.PNG)\n",
    "\n",
    "\n",
    "2. He initalization\n",
    "ReLU 함수에 맞는 초기화 법\n",
    "    * input 갯수의 절반의 제곱근으로 나누어 주면 된다.\n",
    "    * W = np.random.randn(n_input,n_output)/sqrt(n_input/2)\n",
    "https://gomguard.tistory.com/184"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최적화(Optimization) 알고리즘\n",
    "\n",
    "1. SGD\n",
    "    * 손실함수를 계산할 때 전체 Traing set을 사용함\n",
    "    * 계산량이 너무 많아 지는것을 방지\n",
    "    * 전체 데이터에서 일부 데이터의 모음에 대해서만 loss function을 계산\n",
    "    * data를 sampling해서 gradient descent를 진행한다. 이를 반복\n",
    "\n",
    "**loss function 재정리**\n",
    "\n",
    "신경망을 학습할 때 학습 상태에 대해 측청하는 하나의 지표로 사용. 신경망의 가중치 매개변수들이 스스로 특징을 찾아 가기에 이 가중치 값이 최적이 될 수록 해야 하며 잘 찾아가고 있는지 볼때 손실함수를 본다.\n",
    "\n",
    "1) Mean Squared Error(평균제곱오차)\n",
    "\n",
    "    예측 값과 실제 값의 차이를 제곱하여 평균을 낸것\n",
    "* 예측값과 실제값의 차이가 클 수록 평균 오차제곱도 커짐(작아야 좋음)\n",
    "$$ E=\\dfrac {1}{n}\\sum ^{n}_{k}\\left( y_{k}-t_{k}\\right) ^{2} $$\n",
    "    \n",
    "2) Cross entropy error\n",
    "\n",
    "    기본적으로 분류 문제에서 one hot 코딩을 했을 경우에만 사용할 수 있는 오차 계산 법\n",
    "    밑이 자연로그인 e를 예측값에 씌워서 실제 값과 곱한후 전체 값을 합한후 음수로 변환\n",
    "* 엔트로피가 낮을 수록 좋다\n",
    "  $$  E=-\\sum _{k}t_{k}\\log y_{k} $$\n",
    "\n",
    "\n",
    "#### 알고리즘 정리 사이트\n",
    "http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 29,569\n",
      "Trainable params: 29,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 - 0s - loss: 0.6225 - acc: 0.6518 - binary_crossentropy: 0.6225 - val_loss: 0.5906 - val_acc: 0.6835 - val_binary_crossentropy: 0.5906\n",
      "Epoch 2/20\n",
      "25000/25000 - 0s - loss: 0.5835 - acc: 0.6902 - binary_crossentropy: 0.5835 - val_loss: 0.5851 - val_acc: 0.6924 - val_binary_crossentropy: 0.5851\n",
      "Epoch 3/20\n",
      "25000/25000 - 0s - loss: 0.5756 - acc: 0.6980 - binary_crossentropy: 0.5756 - val_loss: 0.5774 - val_acc: 0.6954 - val_binary_crossentropy: 0.5774\n",
      "Epoch 4/20\n",
      "25000/25000 - 0s - loss: 0.5668 - acc: 0.7029 - binary_crossentropy: 0.5668 - val_loss: 0.5884 - val_acc: 0.6867 - val_binary_crossentropy: 0.5884\n",
      "Epoch 5/20\n",
      "25000/25000 - 0s - loss: 0.5613 - acc: 0.7099 - binary_crossentropy: 0.5613 - val_loss: 0.5681 - val_acc: 0.7021 - val_binary_crossentropy: 0.5681\n",
      "Epoch 6/20\n",
      "25000/25000 - 0s - loss: 0.5581 - acc: 0.7114 - binary_crossentropy: 0.5581 - val_loss: 0.5803 - val_acc: 0.6927 - val_binary_crossentropy: 0.5803\n",
      "Epoch 7/20\n",
      "25000/25000 - 0s - loss: 0.5533 - acc: 0.7124 - binary_crossentropy: 0.5533 - val_loss: 0.5659 - val_acc: 0.7039 - val_binary_crossentropy: 0.5659\n",
      "Epoch 8/20\n",
      "25000/25000 - 0s - loss: 0.5496 - acc: 0.7173 - binary_crossentropy: 0.5496 - val_loss: 0.5649 - val_acc: 0.7063 - val_binary_crossentropy: 0.5649\n",
      "Epoch 9/20\n",
      "25000/25000 - 0s - loss: 0.5447 - acc: 0.7225 - binary_crossentropy: 0.5447 - val_loss: 0.5707 - val_acc: 0.7019 - val_binary_crossentropy: 0.5707\n",
      "Epoch 10/20\n",
      "25000/25000 - 0s - loss: 0.5397 - acc: 0.7251 - binary_crossentropy: 0.5397 - val_loss: 0.5669 - val_acc: 0.7048 - val_binary_crossentropy: 0.5669\n",
      "Epoch 11/20\n",
      "25000/25000 - 0s - loss: 0.5354 - acc: 0.7260 - binary_crossentropy: 0.5354 - val_loss: 0.5831 - val_acc: 0.6930 - val_binary_crossentropy: 0.5831\n",
      "Epoch 12/20\n",
      "25000/25000 - 0s - loss: 0.5319 - acc: 0.7312 - binary_crossentropy: 0.5319 - val_loss: 0.5672 - val_acc: 0.7045 - val_binary_crossentropy: 0.5672\n",
      "Epoch 13/20\n",
      "25000/25000 - 0s - loss: 0.5270 - acc: 0.7342 - binary_crossentropy: 0.5270 - val_loss: 0.5725 - val_acc: 0.6999 - val_binary_crossentropy: 0.5725\n",
      "Epoch 14/20\n",
      "25000/25000 - 0s - loss: 0.5232 - acc: 0.7378 - binary_crossentropy: 0.5232 - val_loss: 0.5798 - val_acc: 0.6992 - val_binary_crossentropy: 0.5798\n",
      "Epoch 15/20\n",
      "25000/25000 - 0s - loss: 0.5196 - acc: 0.7391 - binary_crossentropy: 0.5196 - val_loss: 0.5728 - val_acc: 0.7008 - val_binary_crossentropy: 0.5728\n",
      "Epoch 16/20\n",
      "25000/25000 - 0s - loss: 0.5146 - acc: 0.7424 - binary_crossentropy: 0.5146 - val_loss: 0.5870 - val_acc: 0.6936 - val_binary_crossentropy: 0.5870\n",
      "Epoch 17/20\n",
      "25000/25000 - 0s - loss: 0.5117 - acc: 0.7441 - binary_crossentropy: 0.5117 - val_loss: 0.5744 - val_acc: 0.7025 - val_binary_crossentropy: 0.5744\n",
      "Epoch 18/20\n",
      "25000/25000 - 0s - loss: 0.5080 - acc: 0.7463 - binary_crossentropy: 0.5080 - val_loss: 0.5816 - val_acc: 0.6967 - val_binary_crossentropy: 0.5816\n",
      "Epoch 19/20\n",
      "25000/25000 - 0s - loss: 0.5039 - acc: 0.7493 - binary_crossentropy: 0.5039 - val_loss: 0.5847 - val_acc: 0.6973 - val_binary_crossentropy: 0.5847\n",
      "Epoch 20/20\n",
      "25000/25000 - 0s - loss: 0.4980 - acc: 0.7558 - binary_crossentropy: 0.4980 - val_loss: 0.5903 - val_acc: 0.6910 - val_binary_crossentropy: 0.5903\n"
     ]
    }
   ],
   "source": [
    "## SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# 영화의 리뷰에 따라 데이터길이가 다르기 때문에 shape를 맞춰주어야 한다.\n",
    "def sequence_shaping(sequence,dimension):\n",
    "    results = np.zeros((len(sequence),dimension))\n",
    "    for i, word_indices in enumerate(sequence): #enumerate 는 index와 그값 으로 이루어지게 하는\n",
    "        results[i,word_indices] = 1.0 # 각 리뷰의 word_indices 위치에 1대입\n",
    "    return results\n",
    "# 100번째 까지 많이 사용하는 단어 추출\n",
    "word_num = 100\n",
    "data_num = 25000\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=word_num)\n",
    "train_data = sequence_shaping(train_data, dimension = word_num)\n",
    "test_data = sequence_shaping(test_data, dimension = word_num)\n",
    "\n",
    "optimizer_model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation = tf.nn.relu, input_shape =(word_num,)),\n",
    "    keras.layers.Dense(128, activation = tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "optimizer_model.compile(optimizer=optimizers.Adam(lr=0.001,beta_1 = 0.05,beta_2 = 0.05),\n",
    "                   loss = 'binary_crossentropy',\n",
    "                   metrics = ['accuracy','binary_crossentropy'])\n",
    "optimizer_model.summary()\n",
    "\n",
    "optimizer_history = optimizer_model.fit(train_data,train_labels, epochs = 20, batch_size = 500, validation_data=(test_data, test_labels),verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

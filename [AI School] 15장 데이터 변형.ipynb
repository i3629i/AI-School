{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 변형(Agumentation)\n",
    "1. 데이터 변형\n",
    "    1. 데이터 변형 종류\n",
    "    1. Keras이용 Argumentation\n",
    "    1. opencv이용 Argumentation\n",
    "1. 과적합(Overfitting) 방지(Drop out, 정규화)\n",
    "    1. 과적합(overfittion)\n",
    "    1. 드랍아웃(Dorp out)\n",
    "    1. 정규화(Regularzation) L1, L2 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 변형 개념 과 종류\n",
    "\n",
    "Data Argumentation\n",
    "* 데이터를 늘려서 네트워크의 성능을 높이기 위해서 사용하는 방법 ※ 데이터 적을때 사용하면 효과적\n",
    "* 이미지를 여러 방법을 통해 변형 후 네트워크 입력 이미지로 사용하는 방식\n",
    "* 일종의 정규화(Regulazation) 작업으로 과적합을 막는 효과도 있음\n",
    "---\n",
    "#### 종류 \n",
    "1. 평행이동(Translation)\n",
    "    * 이미지의 픽셀을 상하좌우로 이동\n",
    "    * 컴퓨터는 이미지를 이동하면 원본과 다른 것으로 인식\n",
    " \n",
    " \n",
    "2. 좌우대칭(Horizontal Flip)\n",
    "    * 왼쪽만 보는 고양이 사진을 70장 훈련시키면 오른쪽 보는 고양이는 인식을 못함\n",
    "    * 좌우대칭을 시켜줘서 훈련 시켜줘야 맞출수 있음.\n",
    "    \n",
    "\n",
    "3. 랜덤 크랍(Random Crop)\n",
    "    * 예를들어 사람은 고양이나 강이지의 어느 부분만 봐도 고양이인지 알 수 있음.\n",
    "    * Random crop해서 nn에 넣어주면 고양이판단도 가능(부분잘라서 학습)\n",
    "    \n",
    "    \n",
    "4. 밝기 조절\n",
    "    * 조명이나 빛의 반사 등에 밝기 변해도 NN인식 가능\n",
    "    \n",
    "\n",
    "5. 크기 변경\n",
    "    * 이미지 크기가 바뀌어도 NN이 같은 이미지로 인식 가능\n",
    "    * 확대축소는 crop후 Rescale\n",
    "    \n",
    "6. 일부 지우기\n",
    "    * 가려짐에 대응가능\n",
    "    \n",
    "etc. 블러, 컬러노이즈 ,랜덤 노이즈,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 과적합(Overfitting) 방지(Drop out, 정규화)\n",
    "1. 과적합 Overfitting\n",
    "    * 지나치게 학습을 한다는 것을 뜻한다\n",
    "    ![ovefitting](https://user-images.githubusercontent.com/50629716/62823912-d81f4e00-bbd1-11e9-95fc-b160b23d78a8.PNG)\n",
    "    * Train데이터는 실제 데이터의 부분집합인 경우가 대부분이라 학습 데이터에서는 오차가 감소 하지만, Test 데이터 에서는 과하게 학습을 진행하면 오차가 증가하는 지점이 존재할 것이다.\n",
    "    \n",
    "**Overfitting 원인**\n",
    "* 데이터의 분산이 높은 경우\n",
    "* 너무 많은 Epoch로 학습 데이터를 학습하는 경우\n",
    "* 학습에 사용된 파라미터가 너무 많은 경우\n",
    "* 데이터에 노이즈가 많은경우\n",
    "* 복잡한 모델링을 한 경우\n",
    "\n",
    "\n",
    "2. 과소적합 (Underfitting)\n",
    "    * 데이터가 덜 학습되어진 상황\n",
    "    \n",
    "![under onve](https://user-images.githubusercontent.com/50629716/62824081-ea01f080-bbd3-11e9-979a-072d34c89417.PNG)\n",
    "* 과대적합은 너무 많이 학습해서 인간의 범위를 뛰어나게 인지를 해버림 그 학습한 하나만 알 수 있는 상황이 벌어짐\n",
    "* 과소적합은 편향적으로 성격이나 차이를 구분하기 힘든 상황 적은 데이터라 어쩔수 없음\n",
    "\n",
    "https://untitledtblog.tistory.com/68\n",
    "\n",
    "3. Dropout\n",
    "   * 전체 weight을 계산에 참여 시키는 것이 아니라 layer에 포함된 weight중 일부만 참여시키는 것. overfitting에 좋은 성능을 나타낸다.\n",
    "![dropout](https://user-images.githubusercontent.com/50629716/62824172-3568ce80-bbd5-11e9-8130-95c4cf14d5dd.PNG)\n",
    "https://pythonkim.tistory.com/42\n",
    "\n",
    "dropout은 0과 1사이의 값으로, 사용하려고 하는 비율을 말한다. 전체 사용일땐 1값 0.7은70% 사용\n",
    "\n",
    "4. L1 & L2 \n",
    "    * L1 규제는 가중치의 절댓값에 비례하는 비용이 추가됩니다(즉, 가중치의 \"L1 노름(norm)\"을 추가합니다).\n",
    "    \n",
    "    * L2 규제는 가중치의 제곱에 비례하는 비용이 추가됩니다(즉, 가중치의 \"L2 노름\"의 제곱을 추가합니다). 신경망에서는 L2 규제를 가중치 감쇠(weight decay)라고도 부릅니다. 이름이 다르지만 혼돈하지 마세요. 가중치 감쇠는 수학적으로 L2 규제와 동일합니다\n",
    "    \n",
    "    ![L2](https://user-images.githubusercontent.com/50629716/62824442-b1b0e100-bbd8-11e9-8841-5f8209e88046.PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
